\label{sl-nn}
In this chapter we will make a an overview about neural networks and some other concepts  for better explain the following parts of the thesis.

\section{Introduction}
The Artificial Neural Networks is a family of classification technique\footnote{Classification is the operation of learning a function \textbf{$f$} that map example records \textbf{$x \in D$}, where $D$ is the set of \textbf{$(x, y)$}, to the labels set \textbf{$y$} (the classes). This target function is called also Classification Model and is used in a descriptive or predictive way problem depending.}, that are inspired by the human brain: in particular by the connections inside this latter. The human brain consists principally of nerve cells called \textbf{neurons}, that are connected together via the \textbf{axons}\footnote{Inserirne una, se serve} used to transmit the electrical impulse by a neuron to another. This electrical impulse generated by a stimulated neuron is transferred to another one via the dendrites, a particular elements in the human brain used to connect two neuron: this point of contact is called synapse. \newline Analogously the internal structure of a Neural Network is composed by components, which are called \textbf{neurons}, connected together by directed links. There are many types of Neural Networks, but for the sake of simplicity and  for the scope of the thesis we will focus the attention to feedforward neural network model, explaining firstly the Perceptron model.

\section{Perceptron}\label{sec:perceptron}
The Perceptron model is the basic and simplest type of Neural Network, proposed firstly by Frank Rosenblatt in 1958. This model is composed only by two types of nodes called \textbf{input nodes} and \textbf{output node}, as illustrated in the figure \ref{fig:illustration-perceptron-model}. As we can see in this latter the input nodes and the output node are connected by weighted links, similarly to the human brain, that are used to simulate the synaptic connections strength. \newline
\begin{figure}[t]
	\centering

	\begin{tikzpicture}[shorten >=1pt]
		\tikzstyle{unit}=[draw,shape=circle,minimum size=1.15cm]
		%\tikzstyle{hidden}=[draw,shape=circle,fill=black!25,minimum size=1.15cm]
		\tikzstyle{hidden}=[draw,shape=circle,minimum size=1.15cm]
		\tikzstyle{weights}=[draw,shape=rectangle,minimum size=1.15cm]

		\node[unit](x0) at (0,3.5){$x_0$};
		\node[unit](x1) at (0,2){$x_1$};
		\node at (0,1){\vdots};
		\node[unit](xd) at (0,-0.25){$x_N$};

		\node[weights](w0) at (3,3.5){$w_0$};
		\node[weights](w1) at (3,2){$w_1$};
		\node at (3,1){\vdots};
		\node[weights](wd) at (3,-0.25){$w_N$};

		\node[unit](y1) at (6, 2){$\sum\limits_{i}^{n}{x_iw_i}$};

		\node[unit](f) at (9, 2){$sign$};
		
		\node[unit](o) at (11, 2){$o$};

		\draw[->] (x0) -- (w0);
		\draw[->] (x1) -- (w1);
		\draw[->] (xd) -- (wd);
		
		\draw[->] (w0) -- (y1);
		\draw[->] (w1) -- (y1);
		\draw[->] (wd) -- (y1);

		\draw[->] (y1) -- (f);
		
		\draw[->] (f) -- (o);
			
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-0.5,4.25) -- (0.75,4.25) node [black,midway,yshift=+0.6cm]{input layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (2.5,4.25) -- (3.75,4.25) node [black,midway,yshift=+0.6cm]{weights};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (5.5, 3) -- (6.75, 3) node [black,midway,yshift=+0.6cm]{output layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (8.5, 2.75) -- (9.75, 2.75) node [black,midway,yshift=+0.6cm]{activation function};
	\end{tikzpicture}
	\caption{Illustration of the Perceptron model}\label{fig:illustration-perceptron-model}
\end{figure}	


This model calculate the output $\hat{y}$ as a weighted sum of the input respect to the connections weight, to which is summed the bias (a value used to rectify, in this case, the neural network output). So recalling some math, the output of the Perceptron model can be expressed in the following way:
\begin{center}
	$\hat{y} = \textbf{g}(w_{0}x_{0} + w_{1}x_{1} + \dots + w_{i-1}x_{i-1}  + w_{i}x_{i} + b  ) = \textbf{g}(\textbf{w} \bullet \textbf{x})$
\end{center}
where $i = 1, \dots, N$ and $N$ is the cardinality of the vector $\textbf{x}$ and the $\textbf{g}$ acts as the activation function\ref{fig:activation-functions}.
The training of a Perceptron neural network consist in recalculate (the key passage at the step 7 of algorithm \ref{alg:perceptron-learning}), or more precisely adapting, in an iterative manner the weight of the connections until they are able to fit the input data, i.e. the examples $(x, y) \in D$, minimizing or maximizing an objective function.
At the step 7 of the algorithm \ref{alg:perceptron-learning}
\begin{center}
	$w_{j}^{(k + 1)} = w_{j}^{(k)} + \lambda(y_{i} + \hat{y}_{i}^{(k)})x_{ij}$	
\end{center}
we can recognize the $w_{j}^{(k)}$ that is the connection weight of this step, $\lambda$ is the learning rate which is a parameter that instruct the learning algorithm how much quickly the neural network must abandons the old beliefs to substitute them with the new ones and $x_{ij}$ that is the $j^{th}$ value of the $i^{th}$ example.

\begin{algorithm}
	\begin{algorithmic}[1]
		\State{Let $D = \{(\textbf{x}_i, y_i)\ |\ i  = 1, \dots, N\}$ be the set of examples.}
		\State{Initialize the weight vector with random values, $\textbf{w}^{(0)}$}
		\Repeat
			\For{ each example $(\textbf{x}_{i}, y_{i}) \in D$}
				\State Compute the predicted output $\hat{y}_{i}^{(k)}$
				\For{ each weight $w_i$}
					\State Update the weight $w_{j}^{(k + 1)} = w_{j}^{(k)} + \lambda(y_{i} + \hat{y}_{i}^{(k)})x_{ij}$
				\EndFor
			\EndFor
		\Until{ Stopping condition is met }
	\end{algorithmic}
	\caption{Perceptron learning algorithm\cite{ITDM:2014}}\label{alg:perceptron-learning}
\end{algorithm}

\section{Multi-Layer Perceptron (MLP)}
The multilayer perceptron, known as feedforward neural networks, can be seen as an evolution of single layer perceptron\ref{sec:perceptron}, principally created to resolve the problem that this latter has because:
\begin{itemize}
	\item[•] It can not handles a domain $(\textbf{x}, y) \in D$ with more than two classes, because it divides the input data in only two boundaries, as it can be seen in the figure \ref{fig:perceptron-boundaries}, so more dimensions can not be handled
	\item[•] It can not converge if the data is not linearly separable and this leads to reduce the use of Perceptron in many few case (trivially when the data is linearly separable)
\end{itemize}

\begin{figure}[t!]
	\centering
	\begin{tikzpicture}[scale=.6]
		\begin{axis}[
			width=\linewidth, % Scale the plot to \linewidth
    	     	grid=major, % Display a grid
    	      	grid style={dashed,gray!30}, % Set the style
			xlabel=$x_1$,
			ylabel=$x_2$,
		    %xtick=\empty, ytick=\empty
		]
		\addplot [only marks] table {
			1 8
			6 0
			-1 5
		};
		
		\addplot [only marks, mark=o] table {
			-6 -3
			4 -4
			-3 4
		};
		\addplot [domain=-10:10, samples=2, dashed] {-x + 3};

        \legend{Plot}
      \end{axis}
	\end{tikzpicture}
	\caption{Representation of perceptron's decision separation.}\label{fig:perceptron-boundaries}
\end{figure}

As the Perceptron the goal of MLP is to approximate a function $f^{*}$ that should represent the underlying data with which the neural network is trained, e.g. a classifier where $y=f^{*}(\textbf{x})$ associate an example record $\textbf{x}$ to class $y$. \newline \newline
These models, that includes also single layered perceptron, are called feedforward because the informations flow through the input layer, that contains the data from the example \textbf{x}, through all the intermediate layers called \textbf{hidden layer} and finally to the output layer $\textbf{y}$. So all these levels are connected only with next  and what is missing are feedback connections that fed back the network with the output of neural network, exclusive characteristic of others neural networks called \textbf{recurrent neural networks}. 

To all of these layer are associated different functions, called \textbf{activation function}, that permits to the layers to produce output values that are nonlinear respect the input parameters. Thus, in an other perspective, a feedforward neural networks can be seen as a chain of activation functions.

\begin{figure}[t]
	\centering
	\begin{tikzpicture}[shorten >=1pt]
		\tikzstyle{unit}=[draw,shape=circle,minimum size=1.15cm]
		%\tikzstyle{hidden}=[draw,shape=circle,fill=black!25,minimum size=1.15cm]
		\tikzstyle{hidden}=[draw,shape=circle,minimum size=1.15cm]

		\node[unit](x0) at (0,3.5){$x_0$};
		\node[unit](x1) at (0,1.75){$x_1$};
		%\node at (0,1.075){\vdots};
		\node[unit](xd) at (0,0){$x_D$};

		\node[hidden](h10) at (3,3.5){$y_0^{(1)}$};
		\node[hidden](h11) at (3,1.75){$y_1^{(1)}$};
		%\node at (3,1.075){\vdots};
		\node[hidden](h1m) at (3,0){$y_{n^{(1)}}^{(1)}$};
		
		\node[hidden](hL0) at (6,3.5){$y_0^{(2)}$};
		\node[hidden](hL1) at (6,1.75){$y_1^{(2)}$};
		%\node at (6,1.075){\vdots};
		\node[hidden](hLm) at (6,0){$y_{n^{(2)}}^{(2)}$};

		\node[unit](y1) at (9,2.625){$c_1^{(2)}$};
		\node[unit](y2) at (9,00.875){$c_2^{(2)}$};
		%\node at (9,1.075){\vdots};	
		%\node[unit](yc) at (9,0){$c_C^{(2)}$};

		
		\draw[->] (x0) -- (h10);
		\draw[->] (x0) -- (h11);
		\draw[->] (x0) -- (h1m);

		\draw[->] (x1) -- (h10);
		\draw[->] (x1) -- (h11);
		\draw[->] (x1) -- (h1m);

		\draw[->] (xd) -- (h10);
		\draw[->] (xd) -- (h11);
		\draw[->] (xd) -- (h1m);

		\draw[->] (hL0) -- (y1);
		\draw[->] (hL0) -- (y2);
		%\draw[->] (hL0) -- (yc);

		\draw[->] (hL1) -- (y1);
		\draw[->] (hL1) -- (y2);
		%\draw[->] (hL1) -- (yc);

		\draw[->] (hLm) -- (y1);
		\draw[->] (hLm) -- (y2);
		%\draw[->] (hLm) -- (yc);

		\draw[->] (h10) -- (hL0);
		\draw[->] (h11) -- (hL0);
		\draw[->] (h1m) -- (hL0);
		
		\draw[->] (h10) -- (hL1);
		\draw[->] (h11) -- (hL1);
		\draw[->] (h1m) -- (hL1);
		
		\draw[->] (h10) -- (hLm);
		\draw[->] (h11) -- (hLm);
		\draw[->] (h1m) -- (hLm);
		
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-0.5,4) -- (0.75,4) node [black,midway,yshift=+0.6cm]{input layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (2.5,4) -- (3.75,4) node [black,midway,yshift=+0.6cm]{$1^{\text{st}}$ hidden layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (5.5,4) -- (6.75,4) node [black,midway,yshift=+0.6cm]{$2^{\text{nd}}$ hidden layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (8.5,3.125) -- (9.75,3.125) node [black,midway,yshift=+0.6cm]{output layer};
	\end{tikzpicture}
	\caption[Graph representation of feedforward neural network.]{Graph representation of feedforward neural network with $(2)$-layer, with $D$ input units and $2$ output units. For simplicity of representation all the labels $w_{ij}$ associated to the edges, that represent the weights/parameters of the neural networks, are omitted.}
	\label{fig:multilayer-perceptron}
\end{figure}

From this description we can see that the major difference between single-layered perceptron and feedforward neural networks is the training strategy. In fact the design of the FFN prevents to approach the training with the same strategy used with the perceptron because we does not have \textit{a priori} the informations regard the desired output of all the hidden layers, so we can not update the weights as we have seen in the perceptron.

\section{Artificial Neural Network training} 
The training of a ANN generally follow two main steps: the initialization of the weights and the recalculation of these latter with an algorithm that is guided by the objective to minimize or maximize an objective function in an iterative manner\footnote{In specific we speak of cost or loss function if the objective is to minimize this latter.}, searching for a global minimum\footnote{Obviously the global minimum is not guaranteed and this depend by problem, the ANN adopted design and the optimization algorithm. Often,  what is found is a local minimum.}.

The specific operations in the second step are variable and depend by the used algorithm, but generally can be subdivided in throw more sub-steps: firstly is calculated the objective function with which, in the second steps, all the weights are recalculated with some strategy/algorithm. The most famous technique to do this is the backpropagation, that we present in the subsection \ref{subsec:backpropagation}, coupled to some optimization algorithm such as Stochastic Gradient Descent and Adam, but is not the only one that exists (as we show in the section \ref{chap:differential-evolution}) and that we have treated in this thesis work.

\begin{algorithm}
	\begin{algorithmic}[1]
		\State{Let $D = \{(\textbf{x}_i, y_i)\ |\ i  = 1, \dots, N\}$ be the set of examples.}
		\State{Let $\lambda$ a small real valued learning rate}
		\State{Initialize the weights vector with random values, $\textbf{w}^{(0)}$}
		\Repeat
			\For{ each batch of $(\textbf{x}_{i}, y_{i}) \in D$}
				\State{Compute the predicted output $\hat{y}_{i}^{(k)}$}
				\State{Compute the gradient $\delta$ for each $w_{ij}$ with backpropagation}
				\For{ each weight $w_{ij}$}
					\State{Update the weight $w_{ij}$ with some strategy using the associated $\delta$ and the learning rate $\lambda$}
				\EndFor
			\EndFor
		\Until{ Convergence is met }
	\end{algorithmic}
	\caption{Flow of a general algorithm based on the gradient for ANNs.}\label{alg:ann-training}
\end{algorithm}

\subsection{Backpropagation}\label{subsec:backpropagation}
Backpropagation is an algorithm introduced in \cite{RUM:1986} used to calculate the contribution in error of each neuron for a batch of data computed with respect to an objective function. This latter must be a function that can be capture the difference between the expected output, i.e. the $y$ associated to each example, and the generated output, i.e. the $\hat{y}$ associated to each example, transforming that in a real value.
An example of objective function is the Total Sum of Squared errors:
\begin{center}
	\begin{equation}
		TSS = \frac{1}{2}\sum\limits_{i=1}^{N}(y_i - \hat{y}_{i})^{2}
	\end{equation}\label{eq:tss}
\end{center}
The backpropagation is divided into two phases, the forward and the backward propagation.

\subsubsection{Forward propagation}
Intuitively in the forward propagation pass the network is "forward executed", i.e. starting from a batch of input example the calculations for all the layers are executed up to the output layer in a forward manner. \\
Formally, let $D = \{(\textbf{x}_{i}, y_{i})\ |\ i=1,\ \dots ,\ N \}$ the set of examples, $L = \{(l^{(j)}_{h}, b^{(j)}_{h})\ |\ j = 1,\ \dots,\ M,\ $h is the neurons of the layer$\}$ the set of FNN hidden layers and $A = \{\alpha^{(i)}\ |\ i = 1,\ \dots,\ M\}$. Then selected the sample $(\textbf{x}_i, y_i)$ the parameters, i.e. the weights, for the first level are calculated in this way
\begin{center}
	\begin{equation}
	a^{(1)} = \alpha^{(1)}(\sum\limits_{i=1}^{|\textbf{x}|}x_{i}l_{h}^{(1)} + b^{(1)})
	\end{equation}
\end{center}
where $\alpha^{(1)}$ is the activation function of the first layer.\\
Then, as state previously, the computation continue forward for all the others hidden layers as follow
\begin{center}
	\begin{equation}
	a^{(j)} = \alpha^{(j)}(\sum\limits_{n \in a^{(j - 1)}}a^{(j - 1)}_{n}h^{(j)}_{h} + b^{(j)})
	\end{equation}
\end{center}
where the $h^{(j - 1)}$ are the calculated weights of the previous layer of the layer $h^{(j)}$, up to the output layer where we have
\begin{center}
	\begin{equation}
	\hat{y} = \alpha^{(M)}(\sum\limits_{n \in a^{(M - 1)}}a^{(M - 1)}_{n}o + b^{(M)})	
	\end{equation}
\end{center}
where $\hat{y}$ is the neural networks output that will be used in the backward propagation step and $o = h^{M}_{h = C}$.

\subsubsection{Backward propagation}
Once the objective function is computed the backward propagation can be executed. A strictly requirement is that all the activation functions and the objective function must be differentiable, because it uses the technique of Chain Rule of calculus which, basically, computes the derivatives of functions formed by composing other functions whose derivatives are known.
This make it possible to calculate the error for each node starting from the output layer in a backward mode, i.e. for each layers, starting from the output layer, it is calculated the $\delta$ for all the weights which is contribute to the ANN errors, named also the gradient.

Make it clearer with an example where we calculate the gradient $\delta$ for a generic parameter of the FNN, so let \textbf{TSS} the loss function in \ref{eq:tss} and $w_{ij}$ a parameter\footnote{The connection between two neurons.} for the neuron $j^{th}$ of $l$ arriving from the $i^{th}$ neuron of the previous layer $l-1$. Starting examining the partial derivative of error respect to the parameter $w_{ij}$ applying the chain rule
\begin{center}
	\begin{equation}
	\frac{\partial{TSS}}{\partial{w_{ij}}} = \frac{\partial{TSS}}{\partial{neur_j}}\frac{\partial{neur_j}}{\partial{\alpha_j}}\frac{\partial{\alpha_j}}{\partial{w_{ij}}}
	\end{equation}\label{eq:partial-derivative-w_ij}
\end{center}

Now let's examine the parts of this partial derivative, we have for the first term
\begin{center}
	\begin{equation}
		\frac{\partial{TSS}}{\partial{neur_j}} = \frac{\partial{TSS}}{\partial{y}} = \frac{\partial{}}{\partial{y}}\frac{1}{2}(t - y)^2 = y - t
	\end{equation}\label{eq:partial-derivative-output-layer}
\end{center}
if the $neur_j = y$, i.e. we are in the output layer, otherwise we must use another way if we are in an other layer. So consider all the neurons $N_i = \{u, v, \dots, w\}$ of the $i-th$ layer that receive an input from the neuron $neur_j$ of the previous layer, then we have a recursive expression
\begin{center}
	\begin{equation}
		\frac{\partial{TSS}}{\partial{neur_i}} = \sum\limits_{j \in N_i}\Bigg(\frac{\partial{TSS}}{\partial\alpha_j}\frac{\partial{\alpha_j}}{\partial{neur_i}}\Bigg) = \sum\limits_{j \in N_i}\Bigg(\frac{\partial{TSS}}{\partial{neur_j}}\frac{\partial{neur_j}}{\partial{\alpha_j}}w_{ij}\Bigg)
	\end{equation}\label{eq:partial-derivative-inner-layer}
\end{center}

Then examine the second term and we have
\begin{center}
	\begin{equation}
		\frac{\partial{neur_j}}{\partial{\alpha_j}} = \frac{\partial{}}{\partial{\alpha_j}}\phi(\alpha_j)
	\end{equation}
\end{center}
that as we can observe it's only the derivative of the neuron respect to the activation function $\alpha_j$ associated to the level. A list of derivative of activation function is in sub-section \ref{subsec:activation-function}.

Finally observe the third term of \ref{eq:partial-derivative-w_ij}
\begin{center}
	\begin{equation}
		\frac{\partial{\alpha_j}}{\partial{w_{ij}}} = \frac{\partial{}}{\partial{w_{ij}}}\Bigg(\sum\limits_{k=1}^n w_{kj}neur_k\Bigg) = \frac{\partial{}}{\partial{w_{ij}}}w_{ij}neur_i = neur_i
	\end{equation}
\end{center}
observing that $neur_i = x_i$, if the $l_j$ is the first layer after the input layer.

So putting all together we have 
\begin{center}
	\begin{equation}
		\frac{\partial{TSS}}{\partial{w_{ij}}} = neur_i\delta_j
	\end{equation}
\end{center}
where, considering the two case in \ref{eq:partial-derivative-output-layer} and \ref{eq:partial-derivative-inner-layer}, we have
\begin{center}
	\begin{equation}
		\delta_j = \frac{\partial{TSS}}{\partial{n_j}}\frac{\partial{n_j}}{\partial{\alpha_j}} = \left.
		\begin{cases}
			(n_j - \hat{y}_j)n_j(1 - n_j),\textrm{ if } n_j \textrm{ is an output neuron} \\
(\sum\limits_{r	\in N_i}\delta_{j}w_{jr})n_{j}(1 - n_{j}), \textrm{ if }n_j\textrm{ is a inner neuron}
		\end{cases}\right.
	\end{equation}
\end{center} 

Once all the $\delta$ are calculated then all the parameters can be updated with one of the existent algorithm, like Stochastic Gradient Descent (\textbf{SGD}) or the recent Adaptive Moment (\textbf{ADAM}).


\subsection{Optimization algorithms gradient based}
Once the the gradients are calculated they can be used to recalculate the associated parameters. To do this exist some specialized optimization algorithms that implement gradient descent.

These algorithm have as objective the research of a global minimum of the cost function with a research that, with a similitude, is similar to the descent of a bowl (that can be associated to the complete plot of the objective function) by an hypothetical ball, where the bottom of the bowl correspond to the global minimum of the function.


%\pgfplotsset{every axis/.append style={tick label style={/pgf/number format/fixed},font=\scriptsize,ylabel near ticks,xlabel near ticks,grid=major}}

\pgfdeclareradialshading{ring}{\pgfpoint{0cm}{0cm}}%
{rgb(0cm)=(1,1,1);
rgb(0.7cm)=(1,1,1);
rgb(0.719cm)=(1,1,1);
rgb(0.72cm)=(0.975,0,0);
rgb(0.9cm)=(1,1,1)}

\definecolor{GDFirst}{RGB}{203,62,63}
\definecolor{GDSecond}{RGB}{238,138,112}
\definecolor{GDThird}{RGB}{245,191,113}
\definecolor{GDFourth}{RGB}{221,220,220}
\definecolor{GDFiveth}{RGB}{179,205,249}
\definecolor{GDSixth}{RGB}{129,165,247}
\definecolor{GDSeventh}{RGB}{82,110,215}
\begin{figure}[!ht]
     \subfloat[The Gradient Descent can be seen as the descent of a bowl by a ball. The end of each arrow is a descent step of an hypothetical ball.\label{subfig-1:gradient-descent-1}]{%
		\begin{tikzpicture}[samples=100,smooth, scale=.9]
			\begin{scope}
				\draw[opacity=0,fill=GDFirst,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(20/(sin(2*\x)+2))},{sin(\x)*sqrt(20/(sin(2*\x)+2))});
				\draw[opacity=0,fill=GDSecond,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(16/(sin(2*\x)+2))},{sin(\x)*sqrt(16/(sin(2*\x)+2))});
				\draw[opacity=0,fill=GDThird,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(12/(sin(2*\x)+2))},{sin(\x)*sqrt(12/(sin(2*\x)+2))});
				\draw[opacity=0,fill=GDFourth,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(8/(sin(2*\x)+2))},{sin(\x)*sqrt(8/(sin(2*\x)+2))});
				\draw[opacity=0,fill=GDFiveth,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(4/(sin(2*\x)+2))},{sin(\x)*sqrt(4/(sin(2*\x)+2))});
				\draw[opacity=0,fill=GDSixth,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(1/(sin(2*\x)+2))},{sin(\x)*sqrt(1/(sin(2*\x)+2))});
				\draw[opacity=0,fill=GDSeventh,fill opacity=1] plot[domain=0:360] ({cos(\x)*sqrt(0.0625/(sin(2*\x)+2))},{sin(\x)*sqrt(0.0625/(sin(2*\x)+2))});
			
				\draw[->,blue,ultra thick] (-2,3.65) to (-1.93,3);
				\draw[->,blue,ultra thick] (-1.93,3) to (-1.75,2.4);
				\draw[->,blue,ultra thick] (-1.75,2.4) to (-1.5,1.8);
				\draw[->,blue,ultra thick] (-1.5,1.8) to (-1.15,1.3);
			
				%\node at (-1.4,3.8){\scriptsize $w[0]$};
				%\node at (-1.2,3.2){\scriptsize $w[1]$};
				%\node at (-1.05,2.6){\scriptsize $w[2]$};
				%\node at (-0.8,2){\scriptsize $w[3]$};
				%\node at (-0.6,1.4){\scriptsize $w[4]$};
			\end{scope}
		\end{tikzpicture}     
	}
     \hfill
     \subfloat[Second example plot of gradient descent. As we can see is not certain that the global maximum can be reached. \label{subfig-2:gradient-descent-2}]{%
\begin{tikzpicture}[samples=100,smooth,scale=.9]
			\begin{scope}
				\clip(-4,-1) rectangle (4,4);
				\draw plot[domain=0:360, fill={rgb:red,203;green,67;blue,63}] ({cos(\x)*sqrt(20/(sin(2*\x)+2))},{sin(\x)*sqrt(20/(sin(2*\x)+2))});
				\draw plot[domain=0:360] ({cos(\x)*sqrt(16/(sin(2*\x)+2))},{sin(\x)*sqrt(16/(sin(2*\x)+2))});
				\draw plot[domain=0:360] ({cos(\x)*sqrt(12/(sin(2*\x)+2))},{sin(\x)*sqrt(12/(sin(2*\x)+2))});
				\draw plot[domain=0:360] ({cos(\x)*sqrt(8/(sin(2*\x)+2))},{sin(\x)*sqrt(8/(sin(2*\x)+2))});
				\draw plot[domain=0:360] ({cos(\x)*sqrt(4/(sin(2*\x)+2))},{sin(\x)*sqrt(4/(sin(2*\x)+2))});
				\draw plot[domain=0:360] ({cos(\x)*sqrt(1/(sin(2*\x)+2))},{sin(\x)*sqrt(1/(sin(2*\x)+2))});
				\draw plot[domain=0:360] ({cos(\x)*sqrt(0.0625/(sin(2*\x)+2))},{sin(\x)*sqrt(0.0625/(sin(2*\x)+2))});
			
				\draw[->,blue,ultra thick] (-2,3.65) to (-1.93,3);
				\draw[->,blue,ultra thick] (-1.93,3) to (-1.75,2.4);
				\draw[->,blue,ultra thick] (-1.75,2.4) to (-1.5,1.8);
				\draw[->,blue,ultra thick] (-1.5,1.8) to (-1.15,1.3);
			
				\node at (-1.4,3.8){\scriptsize $w[0]$};
				node at (-1.2,3.2){\scriptsize $w[1]$};
				\node at (-1.05,2.6){\scriptsize $w[2]$};
				\node at (-0.8,2){\scriptsize $w[3]$};
				\node at (-0.6,1.4){\scriptsize $w[4]$};
			\end{scope}
		\end{tikzpicture}}
     \label{fig:dummy}
   \end{figure}

This research of a better parameter configuration not only work with the discovered delta, but also with a parameter $\lambda$ called learning rate. This latter must be small for a better research, because otherwise the optimization algorithm could be direct to a local minimum than to the global, and not too small because otherwise it leads to a slow convergence. So to all weights are not associated only $\delta$ but 
\begin{center}
	\begin{equation}
		\Delta{w_{ij}} = -\lambda\frac{\partial{E}}{\partial{w_{ij}}}
	\end{equation}\label{eq:delta-learning-rate}
\end{center}

\subsubsection*{Stochastic Gradient Descent (SGD)}
Using the equation \ref{eq:delta-learning-rate} the parameters are updated as follows
\begin{center}
	\begin{equation}
		w_{ij}^{(t + 1)} = w_{ij}^{(t)} - \lambda\frac{\partial{E}}{\partial{w_{ij}}} 
	\end{equation}
\end{center}

\subsubsection*{Adaptive Moment (ADAM)}
Let $\beta_1$ and $\beta_2$ exponential decay rates used for the moment estimates, $m_0$ the $1^{st}$ moment vector, $v_0$ the $2^{nd}$ moment vector and $\epsilon$ a small real valued scalar we have 
\begin{center}
	\begin{equation}
		m_{ij}^{(t)} = \beta_1m_{ij}^{(t-1)} + (1 - \beta_1)\delta_{ij}^{(t)}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		v_{ij}^{(t)} = \beta_2v_{ij}^{(t-1)} + (1 - \beta_2)\delta_{ij}^{2,  (t)}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		\hat{m}_{ij}^{(t)} = \frac{m_{ij}^{(t)}}{1 - \beta_{1}}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		\hat{v}_{ij}^{(t)} = \frac{v_{ij}^{(t)}}{1 - \beta_{2}}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		w_{ij}^{(t)} = w_{ij}^{(t)} - \lambda\frac{\hat{m}_{ij}^{(t)}}{\sqrt{\hat{v}_{ij}^{(t)}} + \epsilon}
	\end{equation}
\end{center}

%\section{Recurrent neural networks}
% Write this section if there is remaining time