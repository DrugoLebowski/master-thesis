\label{sl-nn}
In this chapter we will make a an overview about neural networks and some other concepts  for better explain the following parts of the thesis.

\section{Introduction}
The Neural Networks is a family of classification technique\footnote{Classification is the operation of learning a function \textbf{$f$} that map example records \textbf{$x \in D$}, where $D$ is the set of \textbf{$(x, y)$}, to the labels set \textbf{$y$} (the classes). This target function is called also Classification Model and is used in a descriptive or predictive way problem depending.}, that are inspired by the human brain: in particular by the connections inside this latter. The human brain consists principally of nerve cells called \textbf{neurons}, that are connected together via the \textbf{axons}\footnote{Inserirne una, se serve} used to transmit the electrical impulse by a neuron to another. This electrical impulse generated by a stimulated neuron is transferred to another one via the dendrites, a particular elements in the human brain used to connect two neuron: this point of contact is called synapse. \newline Analogously the internal structure of a Neural Network is composed by components, which are called \textbf{neurons}, connected together by directed links. There are many types of Neural Networks, but for the sake of simplicity and  for the scope of the thesis we will focus the attention to feedforward neural network model, explaining firstly the Perceptron model.
\subsection{	Perceptron}
The Perceptron model is the basic and simplest type of Neural Network, proposed firstly by Frank Rosenblatt in 1958. This model is composed only by two types of nodes called \textbf{input nodes} and \textbf{output node}, as illustrated in the figure \ref{fig:perceptron-1}. As we can see in this latter the input nodes and the output node are connected by weighted links, similarly to the human brain, that are used to simulate the synaptic connections strength. \newline
\begin{figure}[t]
	\centering

	\begin{tikzpicture}[shorten >=1pt]
		\tikzstyle{unit}=[draw,shape=circle,minimum size=1.15cm]
		%\tikzstyle{hidden}=[draw,shape=circle,fill=black!25,minimum size=1.15cm]
		\tikzstyle{hidden}=[draw,shape=circle,minimum size=1.15cm]
		\tikzstyle{weights}=[draw,shape=square,minimum size=1.15cm]

		\node[unit](x0) at (0,3.5){$x_0$};
		\node[unit](x1) at (0,2){$x_1$};
		\node at (0,1){\vdots};
		\node[unit](xd) at (0,-0.25){$x_N$};

		\node[weights](w0) at (3,3.5){$w_0$};
		\node[weights](w1) at (3,2){$w_1$};
		\node at (3,1){\vdots};
		\node[weights](wd) at (3,-0.25){$w_N$};

		\node[unit](y1) at (6, 2){$o$};

		\node[unit](f) at (9, 2){$f$};

		\draw[->] (x0) -- (w0);
		\draw[->] (x1) -- (w1);
		\draw[->] (xd) -- (wd);
		
		\draw[->] (w0) -- (y1);
		\draw[->] (w1) -- (y1);
		\draw[->] (wd) -- (y1);

		\draw[->] (y1) -- (f);
		
		\draw[->] (f) -- (10, 2);
			
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-0.5,4.25) -- (0.75,4.25) node [black,midway,yshift=+0.6cm]{input layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (2.5,4.25) -- (3.75,4.25) node [black,midway,yshift=+0.6cm]{weights};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (5.5, 2.75) -- (6.75, 2.75) node [black,midway,yshift=+0.6cm]{output layer};
		\draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (8.5, 2.75) -- (9.75, 2.75) node [black,midway,yshift=+0.6cm]{activation function};
	\end{tikzpicture}
	\caption{Illustration of the Perceptron model}\label{fig:illustration-perceptron-model}
\end{figure}	


This model calculate the output $\hat{y}$ as a weighted sum of the input respect to the connections weight, to which is summed the bias (a value used to rectify, in this case, the neural network output). So recalling some math, the output of the Perceptron model can be expressed in the following way:
\begin{center}
	$\hat{y} = \textbf{g}(w_{0}x_{0} + w_{1}x_{1} + \dots + w_{i-1}x_{i-1}  + w_{i}x_{i} + b  ) = \textbf{g}(\textbf{w} \bullet \textbf{x})$
\end{center}
where $i = 1, \dots, N$ and $N$ is the cardinality of the vector $\textbf{x}$ and the $\textbf{g}$ acts as the activation function\footnote{}.
The training of a Perceptron neural network consist in recalculate (the key passage at the step 7 of algorithm \ref{alg:perceptron-learning}), or more precisely adapting, in an iterative manner the weight of the connections until they are able to fit the input data, i.e. the examples $(x, y) \in D$. 
At the step 7 of the algorithm \ref{alg:perceptron-learning}
\begin{center}
	$w_{j}^{(k + 1)} = w_{j}^{(k)} + \lambda(y_{i} + \hat{y}_{i}^{(k)})x_{ij}$	
\end{center}
we can recognize the $w_{j}^{(k)}$ that is the connection weight of this step, $\lambda$ that is the learning rate that is a parameter that instruct the learning algorithm to how quickly the neural network must abandons the old beliefs to substitute them with the new ones and $x_{ij}$ that is the $j^{th}$ value of the $i^{th}$ example.

\begin{algorithm}
	\caption{Perceptron learning algorithm\cite{ITDM:2014}}\label{alg:perceptron-learning}
	\begin{algorithmic}[1]
		\State Let $D = {(\textbf{x}_i, y_i) | i  = 1, \dots, N}$ be the set of examples.
		\Repeat
			\For{ each example $(\textbf{x}_i, y_i) \in D$}
				\State Compute the predicted output $\hat{y}_i^(k)$
				\For{ each weight $w_i$}
					\State Update the weight $w_{j}^{(k + 1)} = w_{j}^{(k)} + \lambda(y_{i} + \hat{y}_{i}^{(k)})x_{ij}$
				\EndFor
			\EndFor
		\Until{ Stopping condition is met }
	\end{algorithmic}
\end{algorithm}

\subsection{Multi Layer Perceptron (MLP)}
