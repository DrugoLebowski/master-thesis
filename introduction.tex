The success of Deep Learning is undeniable. In the last past years there was an explosion of studies aimed to search new models, like those aimed to image recognition \cite{Krizhevsky2012ImageNetCW}, parsing \cite{Vinyals2015GrammarAA} and speech recognition \cite{Chan2015ListenAA} where the models act on inputs of fixed length. This situation has been reversed at the same time with a new family of model, introduced by the Neural Turing Machine (NTM) \cite{Graves2014NeuralTM} where there is an attempt to train an deep network with few parameters and large memory on new types of problems, such as algorithm recognition. The success of NTM have opened the way to many others similar models, such as Neural GPUs \cite{Kaiser2015NeuralGL}, Neural Programmer \cite{Neelakantan2015NeuralPI}, Neural Programmer-Interpreters \cite{Reed2015NeuralP}, Memory Networks \cite{Weston2014MemoryN}, Stack-Augmented Neural Networks \cite{Joulin2015InferringAP}, Differential Neural Computer \cite{Graves2016HybridCU} and Pointer Network \cite{Vinyals2015PointerN}.
\newline \newline
In this trend, we take into account as a basis of this thesis another interesting model that takes inspiration by the NTM, the Neural Random-Access Machine (NRAM). This model proposed in \cite{NRAM:2016} attempts to translate the concepts of pointer manipulation and dereferencing used in modern computers, through the addition of primitive operations. Obviously, this fact does not means that NRAM is better respect to other computational models. In fact, as specified in \cite{NRAM:2016}
\begin{quote}
[...] the number of timesteps that a given model has is highly limited, as extremely deep models are very difficult to train. As a result, the modelâ€™s core primitives have a strong effect on the set of functions that can be feasibly learned in practice, similarly to the way in which the choice of a programming language strongly affects the functions that can be implemented with an extremely small amount of code. [...]
\end{quote}

The original NRAM implementation is based on backpropagation, pushing it on new level of complexity. Due to the results, we have tried to approach the optimization of this model through the Differential Evolution. Differently from backpropagation and gradient-based algorithm, Differential Evolution faces it in account the optimization problem in a different way. Indeed, interleaves phases of exploitation and exploration. So using the framework DENN, introduced in Section \ref{sec:DENN}, we have completely re-developed the NRAM model and tested it, comparing the results with those resulting from the execution of the gradient-based algorithm ADAM.\newline\newline
The thesis is organized as follows: in Chapter \ref{chap:nram} we introduce the NRAM model; in Chapter \ref{chap:ann} we explain what an Artificial Neural Network is and how they can be trained with the Gradient Descent algorithms; in Chapter \ref{chap:differential-evolution} we introduce Differential Evolution and DENN as alternatives to Gradient Descent algorithms; in Chapter \ref{chap:implementation} we quickly describe the implementation and finally, in Chapter \ref{experiments} we present the results of the experiments.