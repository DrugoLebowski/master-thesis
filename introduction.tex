The success of Deep Learning is undeniable. In the last past years there was an explosion of studies aimed to research new models, like those aimed to image recognition \cite{Krizhevsky2012ImageNetCW}, parsing \cite{Vinyals2015GrammarAA} and speech recognition \cite{Chan2015ListenAA} where the models act on inputs of fixed length. This situation has been reversed at the same time with a new family of model, introduced by the Neural Turing Machine (NTM) \cite{Graves2014NeuralTM} where there is an attempt to train an extremely deep network with few parameters and large memory on new types of problems, such as algorithm recognition. The success of NTM have opened the way to many others similar models, such as Neural GPUs \cite{Kaiser2015NeuralGL}, Neural Programmer \cite{Neelakantan2015NeuralPI}, Neural Programmer-Interpreters \cite{Reed2015NeuralP}, Memory Networks \cite{Weston2014MemoryN}, Stack-Augmented Neural Networks \cite{Joulin2015InferringAP}, Differential Neural Computer \cite{Graves2016HybridCU} and Pointer Network \cite{Vinyals2015PointerN}.
\newline \newline
This is also translated in the model taking into account in this thesis, the Neural Random-Access Machine (NRAM) \cite{NRAM:2016}. This model attempts to translate the concepts of pointer manipulation and dereferencing used in modern computers, through the addition of primitive operations. Obviously, this fact does not means that NRAM is better respect to other computational models. In fact, as specified in \cite{NRAM:2016}
\begin{quote}
[...] the number of timesteps that a given model has is highly limited, as extremely deep models are very difficult to train. As a result, the modelâ€™s core primitives have a strong effect on the set of functions that can be feasibly learned in practice, similarly to the way in which the choice of a programming language strongly affects the functions that can be implemented with an extremely small amount of code. [...]
\end{quote}

The original NRAM implementation is based on backpropagation, pushing it on new level of complexity. Due to the interesting results, we have tried to approach the optimization of this model in another way: trough Differential Evolution. Differently to backpropagation and gradient-based algorithm, Differential Evolution take into account the optimization problem in a different way. In fact, respects to backpropagation, DE takes in account the optimization problem interleaving phases of exploitation and exploration. So using the framework DENN, introduced in Section \ref{sec:DENN}, we have completely re-developed the NRAM model and tested it, comparing the results with those resulting from the execution of ADAM.\newline\newline
The thesis is organized as follows: in Chapter \ref{chap:nram} we introduce the NRAM model; in Chapter \ref{chap:ann} we explain what is an Artificial Neural Network and how they can be trained with the Gradient Descent algorithms; in Chapter \ref{chap:differential-evolution} we introduce Differential Evolution and DENN as alternatives to Gradient Descent algorithms; in Chapter \ref{chap:implementation} we speak quickly about the implementation and finally, in Chapter \ref{experiments} we present the results of the experiments.