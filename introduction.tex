The success of Deep Learning is undeniable. In the past years there has been an explosion of studies aimed to search new models, such as those aimed to image recognition \cite{Krizhevsky2012ImageNetCW}, parsing \cite{Vinyals2015GrammarAA} and speech recognition \cite{Chan2015ListenAA} where the models act on inputs of fixed length. At the same time a new family of models, based on the Neural Turing Machine (NTM) \cite{Graves2014NeuralTM} has been introduced. Such models represent an attempt to train a deep network with few parameters and large memory on new types of problems, such as algorithm recognition. The success of NTM has opened the way to many other similar models, such as Neural GPUs \cite{Kaiser2015NeuralGL}, Neural Programmer \cite{Neelakantan2015NeuralPI}, Neural Programmer-Interpreters \cite{Reed2015NeuralP}, Memory Networks \cite{Weston2014MemoryN}, Stack-Augmented Neural Networks \cite{Joulin2015InferringAP}, Differential Neural Computer \cite{Graves2016HybridCU} and Pointer Network \cite{Vinyals2015PointerN}.
\newline \newline
In this thesis we consider an interesting model that takes inspiration from the NTM, the Neural Random-Access Machine (NRAM). This model, proposed in \cite{NRAM:2016}, attempts to implement the concepts of pointer manipulation and dereferencing through primitive operations.
Hence, NRAM is proposed as an alternative to NTM, holding the same expressive potential while enhancing it through the use of different powerful operations. However, this does not mean that NRAM is better than NTM or the other models based on NTM. In fact, as specified in \cite{NRAM:2016}
\begin{quote}
[...] \textit{As a result, the modelâ€™s core primitives have a strong effect on the set of functions that can be feasibly learned in practice, similarly to the way in which the choice of a programming language strongly affects the functions that can be implemented with an extremely small amount of code.} [...]
\end{quote}

The original NRAM implementation is based on backpropagation, pushing it to a new level of complexity. Starting from \cite{NRAM:2016}, we tried to approach the optimization of this model through the Differential Evolution optimization algorithm. Differently from back-propagation and gradient-based algorithms, the Differential Evolution interleaves phases of exploitation and exploration in the space of the solutions. On the basis of the DENN framework, introduced in Section \ref{sec:DENN}, we have completely re-implemented the NRAM model and tested it. We compared the results with those obtained by the original model optimized by the gradient-based algorithm ADAM.\newline\newline
The thesis is organized as follows: in Chapter \ref{chap:nram} we introduce the NRAM model; in Chapter \ref{chap:ann} we explain what Artificial Neural Networks are and how can they be trained with the Gradient Descent algorithms; in Chapter \ref{chap:differential-evolution} we introduce Differential Evolution and DENN as alternatives to Gradient Descent algorithms; in Chapter \ref{chap:implementation} we describe the implementation and finally, in Chapter \ref{experiments}, we present the results of the experiments.