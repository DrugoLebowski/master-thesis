% TODO: Fare un'introduzione parlando dello stato dell'arte di questo genere di architetture, spiegare cosa va a risolvere la NRAM, fare un confronto con altre soluzioni e brevemente dire cosa la mia tesi punta a risolvere e come
The success of Deep Learning is undeniable and this have opened new ways. In the last past years these have been translated into numerous supervised solutions, as image recognition \cite{Krizhevsky2012ImageNetCW}, parsing \cite{Vinyals2015GrammarAA} and speech recognition \cite{Chan2015ListenAA}, where the models act on input of fixed length. This situation has been reversed at the same time with a new family of model, introduced by the Neural Turing Machine (NTM) \cite{Graves2014NeuralTM} where there is an attempt to train an extremely deep network with few parameters and large memory on new types of problems, such as algorithm recognition. The success of NTM have opened the way to many others similar models, such as Neural GPUs \cite{Kaiser2015NeuralGL}, Neural Programmer \cite{Neelakantan2015NeuralPI}, Neural Programmer-Interpreters \cite{Reed2015NeuralP}, Memory Networks \cite{Weston2014MemoryN}, Stack-Augmented Neural Networks \cite{Joulin2015InferringAP}, Differential Neural Computer \cite{Graves2016HybridCU} and Pointer Network \cite{Vinyals2015PointerN}.
\newline \newline
This is also translated in the model taking into account in this thesis, the Neural Random-Access Machine (NRAM) \cite{NRAM:2016}. This model attempts to translate the concepts of pointer manipulation and dereferencing used in modern computers, through the addition of primitive operations. Obviously, this fact does not means that NRAM is better respect others computational models. In fact, as specified in \cite{NRAM:2016}
\begin{quote}
[...] the number of timesteps that a given model has is highly limited, as extremely deep models are very difficult to train. As a result, the modelâ€™s core primitives have a strong effect on the set of functions that can be feasibly learned in practice, similarly to the way in which the choice of a programming language strongly affects the functions that can be implemented with an extremely small amount of code. [...]
\end{quote}

The original NRAM implementation is based on backpropagation, pushing it on new level of complexity. Due to the interesting results, we have tried to approach the optimization of this model in another way: trough Differential Evolution. Differently to backpropagation and gradient-based algorithm, Differential Evolution take into account the optimization problem in a different way. In fact, respects to backpropagation, DE takes in account the optimization problem interleaving of phases of exploitation and exploration. So using the framework DENN, introduced quickly in Section \ref{sec:DENN}, we have completely re-developed the NRAM model and tested it, comparing the results with those resulting from the backpropagation application.