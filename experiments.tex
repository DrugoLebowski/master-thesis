\label{experiments}
In this chapter we present the problems with which the two solutions of NRAM have been trained, finishing by comparing the results and by presenting the circuits learned by the neural networks.

\section{Tasks}
The following are the description of the executed task used in our experiments. All except the last are the same of \cite{NRAM:2016}. In the description, big and small letters represents respectively arrays and pointers, \textit{NULL} denotes the value 0 and is used as an ending character or in the lists, as a placeholder for missing next element.

\begin{itemize}
	\item[1]{\textbf{Access} Given a value $k$ and an array \textbf{A}, return $\textbf{A}[k]$. Input is given as $k,\ A[0],\ \dots,\ $\\$\textbf{A}[n-1],\ \textit{NULL}$ and the network should replace the first memory cell with $\textbf{A}[k]$.}
	\item[2]{\textbf{Increment} Given an array $\textbf{A}$, increment all its elements by 1. Input is given as $\textbf{A}[0],\ \dots,\ \textbf{A}[n-1],\ \textit{NULL}$ and the expected output is $\textbf{A}[0] + 1,\ \dots,\ A[n-1] + 1$.}
	\item[3]{\textbf{Copy} Given an array and a pointer to the destination, copy all elements from the array to the given location. Input is given as $p,\ \textbf{A}[0],\ \dots,\ \textbf{A}[n-1]$ where $p$ points to one element after $\textbf{A}[n-1]$. The expected output is $\textbf{A}[0],\ \dots,\ \textbf{A}[n-1]$ at positions $p,\ \dots,\ p+n-1$ respectively.}
	\item[4]{\textbf{Reverse} Given an array and a pointer to the destination, copy all elements from the array in reversed order. Input is given as $p,\ \textbf{A}[0],\ \dots,\ \textbf{A}[n-1]$ where $p$ points one element after $\textbf{A}[n-1]$. The expected output is $\textbf{A}[n-1],\ \dots,\ \textbf{A}[0]$ at positions $p,\ \dots,\ p+n-1$ respectively.}
	\item[5]{\textbf{Swap} Given two pointers $p,\ q$ and an array \textbf{A}, swap elements $\textbf{A}[p]$ and $\textbf{A}[q]$. Input is given as $p,\ q,\ \textbf{A}[0],\ \dots,\ \textbf{A}[p],\ \dots,\ \textbf{A}[q],\ \dots,\ \textbf{A}[n-1],\ 0$. The expected modified array \textbf{A} is: $\textbf{A}[0],\ \dots,\ \textbf{A}[q],\ \dots,\ \textbf{A}[p],\ \dots,\ \textbf{A}[n-1]$.}
	\item[6]{\textbf{Permutation} Given two arrays of n elements: P (contains a permutation of numbers $0,\ \dots,\ n-1$) and \textbf{A} (contains random elements), permutate \textbf{A} according to P. Input is given as a, $P[0],\ \dots,\ P[n-1],\ \textbf{A}[0],\ ...,\ \textbf{A}[n-1]$, where a is a pointer to the array \textbf{A}. The expected output is $\textbf{A}[P[0]],\ \dots,\ \textbf{A}[P[n-1]]$, which should override the array P.}
	\item[6]{\textbf{Sum} Given pointers to 2 arrays \textbf{A} and \textbf{B}, and the pointer to the output $o$, sum the two arrays into one array. The input is given as: $a,\ b,\ o,\ \textbf{A}[0],\ \dots,\ \textbf{A}[n-1],\ G,\ \textbf{B}[0],\ \dots,\ \textbf{B}[m-1],\ G$, where $a$ points to first element of \textbf{A}, $b$ points to the first element of \textbf{B}, $o$ points to first element of output array and $G$ is a special guardian value. The $\textbf{A}+\textbf{B}$ array should be written starting from position $o$.}
\end{itemize}

\section{Results}
\begin{table}[]
\centering
\label{my-label} 
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Task} & \textbf{Train complexity} & \textbf{Train error} & \textbf{Generalization} \\ \hline

Access & $\textrm{len}(A) \leq 20$ & 0 & Perfect \\ \hline
Increment & $\textrm{len}(A) \leq 10$ & 0 & Perfect \\ \hline
Copy & $\textrm{len}(A) \leq 6$ & 0 & Perfect \\ \hline
Reverse & $\textrm{len}(A) \leq 6$ & 0 & Perfect \\ \hline
Swap & $\textrm{len}(A) \leq 10$ & 0 & Perfect \\ \hline

\end{tabular}
\caption{Configurations used in tests}
\label{tbl:tests-configuration}
\end{table}


In the experiments we have retraced what is do in the paper \cite{NRAM:2016}, trying to test the learnability on the same ``easy'' and ``hard'' tasks using the Differential Evolution\footnote{The algorithms used are \textbf{JADE}, \textbf{SHADE} and \textbf{L-SHADE}, combined with the mutation methods \textbf{DEGL} and \textbf{Curr to p best} and the crossover method \textbf{bin}.}. This tests are compared then to the implementation with ADAM. We have used always the same configurations of NRAM, like number of registers and maximum integer, among the tests. Hence, to not overload the tables they are showed in the Section \ref{subsec:circuits} associated to the generated circuits. The cost calculation has been done with the cost function showed in the Section \ref{subsec:cost-function}, that evaluates the manipulated input memory respect to the desired memory, according to the cost mask.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{7}{|c|}{\textbf{Access}} \\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Network}} \\ \hline
		Hidden Layer & \multicolumn{6}{c|}{$2 \times 260$}\\ \hline
		H. L. Activation function & \multicolumn{6}{c|}{2 $\times$ ReLu}\\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Differential Evolution}} \\ \hline
		DE type & \multicolumn{2}{c|}{JADE} & \multicolumn{2}{c|}{SHADE} & \multicolumn{2}{c|}{L-SHADE}  \\ \hline
		Mutation & DEGL & C.p.b. & DEGL & C.p.b. & DEGL & C.p.b. \\ \hline
		Crossover & \multicolumn{6}{c|}{bin} \\ \hline
		Population & \multicolumn{6}{c|}{60} \\ \hline
		Training gen. & \multicolumn{6}{c|}{1000} \\ \hline
		Converged to 0 & & & & & & \\ \hline
		F & & & & & &\\ \hline
		CR & & & & & &\\ \hline
		Archive size & \multicolumn{6}{c|}{70} \\ \hline
		Memory \textbf{M} Size & $\times$ & $\times$ & & & &\\ \hline
		DEGL neighbors & 5 & $\times$ & 5 & $\times$ & 5 & $\times$  \\ \hline\hline
		\multicolumn{7}{|c|}{\textbf{ADAM}} \\ \hline
		Converged to 0 & \multicolumn{6}{|c|}{\checkmark} \\ \hline
 	\end{tabular}
	\caption{Best configurations used in the tests of \textbf{Access}}
	\label{tbl:tests-configurations-access}
\end{table}

\paragraph{Easy tasks}
The ``easy'' tasks set includes \textbf{Access}, \textbf{Copy}, \textbf{Increment}, \textbf{Reverse} and \textbf{Swap}. Overall, we have always have found a good set of training hyperparameters for Differential Evolution with which cost zero is achieved. The same is not happened with ADAM, which is not converged on \textbf{Swap}. The used configuration are visible in Tables \ref{tbl:tests-configurations-access}-\ref{tbl:tests-configurations-swap} and training trend in Figures \ref{fig:access-plot}-\ref{fig:swap-plot}. Surprisingly, the NRAM trained with DE over the task Access with a controller without hidden layers converged to zero, as can be seen in Figure \ref{fig:access-plot-no-hidden-layer} and in Table \ref{ref:access-table-no-hidden-layer}; the same result was not obtained with ADAM.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{7}{|c|}{\textbf{Increment}} \\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Network}} \\ \hline
		Hidden Layer & \multicolumn{6}{c|}{$2 \times 260$}\\ \hline
		H. L. Activation function & \multicolumn{6}{c|}{2 $\times$ ReLu}\\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Differential Evolution}} \\ \hline
		DE type & \multicolumn{2}{c|}{JADE} & \multicolumn{2}{c|}{SHADE} & \multicolumn{2}{c|}{L-SHADE}  \\ \hline
		Mutation & DEGL & C.p.b. & DEGL & C.p.b. & DEGL & C.p.b. \\ \hline
		Crossover & \multicolumn{6}{c|}{bin} \\ \hline
		Population & \multicolumn{6}{c|}{60} \\ \hline
		Training gen. & \multicolumn{6}{c|}{1000} \\ \hline
		Converged to 0 & & & & & & \\ \hline
		F & & & & & &\\ \hline
		CR & & & & & &\\ \hline
		Archive size & \multicolumn{6}{c|}{70} \\ \hline
		Memory \textbf{M} Size & $\times$ & $\times$ & & & &\\ \hline
		DEGL neighbors & 5 & $\times$ & 5 & $\times$ & 5 & $\times$  \\ \hline\hline
		\multicolumn{7}{|c|}{\textbf{ADAM}} \\ \hline
		Converged to 0 & \multicolumn{6}{|c|}{\checkmark} \\ \hline
 	\end{tabular}
	\caption{Best configurations used in the tests of \textbf{Increment}}
	\label{tbl:tests-configurations-increment}
\end{table}

\paragraph{Hard tasks} 
The ``hard'' tasks set includes \textbf{Permutation}, \textbf{Merge}, \textbf{ListK}, \textbf{ListSearch}, \textbf{WalkBST} and out custom \textbf{Sum}. Unfortunately, due to lack of time and computational capacity the tests of \textbf{Merge}, \textbf{ListK}, \textbf{ListSearch}, \textbf{WalkBST} were not completed or executed. The performed tests for \textbf{Permutation} and \textbf{Sum} ... %TODO

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{7}{|c|}{\textbf{Copy}} \\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Network}} \\ \hline
		Hidden Layer & \multicolumn{6}{c|}{$2 \times 260$}\\ \hline
		H. L. Activation function & \multicolumn{6}{c|}{2 $\times$ ReLu}\\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Differential Evolution}} \\ \hline
		DE type & \multicolumn{2}{c|}{JADE} & \multicolumn{2}{c|}{SHADE} & \multicolumn{2}{c|}{L-SHADE}  \\ \hline
		Mutation & DEGL & C.p.b. & DEGL & C.p.b. & DEGL & C.p.b. \\ \hline
		Crossover & \multicolumn{6}{c|}{bin} \\ \hline
		Population & \multicolumn{6}{c|}{60} \\ \hline
		Training gen. & \multicolumn{6}{c|}{1000} \\ \hline
		Converged to 0 & & & & & & \\ \hline
		F & & & & & &\\ \hline
		CR & & & & & &\\ \hline
		Archive size & \multicolumn{6}{c|}{70} \\ \hline
		Memory \textbf{M} Size & $\times$ & $\times$ & & & &\\ \hline
		DEGL neighbors & 5 & $\times$ & 5 & $\times$ & 5 & $\times$  \\ \hline\hline
		\multicolumn{7}{|c|}{\textbf{ADAM}} \\ \hline
		Converged to 0 & \multicolumn{6}{|c|}{\checkmark} \\ \hline
 	\end{tabular}
	\caption{Best configurations used in the tests of \textbf{Copy}}
	\label{tbl:tests-configurations-copy}
\end{table}

\paragraph{Generalization}
The generalization tests have been performed with sequences of maximum 100 values, showing that the algorithms are effectively been learned. The error of each task, associated to the complexity, is visible in the Table \ref{tbl:error-hard-tasks} and Figure \ref{fig:error-hard-tasks}.


\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{7}{|c|}{\textbf{Reverse}} \\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Network}} \\ \hline
		Hidden Layer & \multicolumn{6}{c|}{$2 \times 260$}\\ \hline
		H. L. Activation function & \multicolumn{6}{c|}{2 $\times$ ReLu}\\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Differential Evolution}} \\ \hline
		DE type & \multicolumn{2}{c|}{JADE} & \multicolumn{2}{c|}{SHADE} & \multicolumn{2}{c|}{L-SHADE}  \\ \hline
		Mutation & DEGL & C.p.b. & DEGL & C.p.b. & DEGL & C.p.b. \\ \hline
		Crossover & \multicolumn{6}{c|}{bin} \\ \hline
		Population & \multicolumn{6}{c|}{60} \\ \hline
		Training gen. & \multicolumn{6}{c|}{1000} \\ \hline
		Converged to 0 & & & & & & \\ \hline
		F & & & & & &\\ \hline
		CR & & & & & &\\ \hline
		Archive size & \multicolumn{6}{c|}{70} \\ \hline
		Memory \textbf{M} Size & $\times$ & $\times$ & & & &\\ \hline
		DEGL neighbors & 5 & $\times$ & 5 & $\times$ & 5 & $\times$  \\ \hline\hline
		\multicolumn{7}{|c|}{\textbf{ADAM}} \\ \hline
		Converged to 0 & \multicolumn{6}{|c|}{\checkmark} \\ \hline
 	\end{tabular}
	\caption{Best configurations used in the tests of \textbf{Reverse}}
	\label{tbl:tests-configurations-reverse}
\end{table}

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{7}{|c|}{\textbf{Swap}} \\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Network}} \\ \hline
		Hidden Layer & \multicolumn{6}{c|}{$2 \times 260$}\\ \hline
		H. L. Activation function & \multicolumn{6}{c|}{2 $\times$ ReLu}\\ \hline \hline
		\multicolumn{7}{|c|}{\textbf{Differential Evolution}} \\ \hline
		DE type & \multicolumn{2}{c|}{JADE} & \multicolumn{2}{c|}{SHADE} & \multicolumn{2}{c|}{L-SHADE}  \\ \hline
		Mutation & DEGL & C.p.b. & DEGL & C.p.b. & DEGL & C.p.b. \\ \hline
		Crossover & \multicolumn{6}{c|}{bin} \\ \hline
		Population & \multicolumn{6}{c|}{60} \\ \hline
		Training gen. & \multicolumn{6}{c|}{1000} \\ \hline
		Converged to 0 & & & & & & \\ \hline
		F & & & & & &\\ \hline
		CR & & & & & &\\ \hline
		Archive size & \multicolumn{6}{c|}{70} \\ \hline
		Memory \textbf{M} Size & $\times$ & $\times$ & & & &\\ \hline
		DEGL neighbors & 5 & $\times$ & 5 & $\times$ & 5 & $\times$  \\ \hline\hline
		\multicolumn{7}{|c|}{\textbf{ADAM}} \\ \hline
		Converged to 0 & \multicolumn{6}{|c|}{\checkmark} \\ \hline
 	\end{tabular}
	\caption{Best configurations used in the tests of \textbf{Swap}}
	\label{tbl:tests-configurations-swap}
\end{table}

\subsection{Circuits}\label{subsec:circuits}