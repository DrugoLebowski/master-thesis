\label{chap:implementation}
Our thesis work is split in three different projects: a refactoring of the already existing project\footnote{\hyperref[https://github.com/gibiansky/experiments/tree/master/nram]{https://github.com/gibiansky/experiments/tree/master/nram}} of Andrew Gibiansky, written in Python and based on Theano\footnote{Theano is a Python library created at LISA labs of University of Montreal, which lets to define easily complex mathematical expressions and execute them efficiently in the CPU and GPU specially with those which involves multi-dimensional arrays. The expressions definition is made through the creation of computational graph which is then executed and, if requested, automatic differentiated with an automatic differentiator, e.g. when one would calculate the gradient.} library, to which some parts of NRAM and a parametrization system are missing, an implementation in C++ of the NRAM using DENN and additional implementation\footnote{\hyperref[https://github.com/DrugoLebowski/denn-lite-nram-executor]{https://github.com/DrugoLebowski/denn-lite-nram-executor}} of the NRAM without the training phase, used to test the generalization capacity of the discovered ANNs.


\paragraph{Preface}
Because some of the tasks presented in Chapter \ref{experiments} are too complex to be learn by NRAM, we have also used some of the technique that have been used in the original paper \cite{NRAM:2016}.

\subparagraph{Gradient clipping}
The size of the model can be extremely huge and moreover extremely depth, due to the execution in various timesteps. In these types of networks the gradient can often explode, as noticed in \cite{Bengio1994LearningLD}. Hence, in Theano implementation all the gradients are clipped in the range $[-C_1, C_1]$ and successively rescaled, such that its $L_2$\footnote{Let $\textbf{x} = [x_1, x_2, \dots, x_n]$, the norm $L_2$ is defined as $|\textbf{x}| = \sqrt{\sum\limits_{k=1}^{n}|x_i|^2}$.} norm is not bigger than some constant $C_2$.

\subparagraph{Noise}
As noticed in \cite{Neelakantan2015AddingGN}, in Theano implementation is added a Gaussian noise (whose variance decays over times) to the computed gradient.

\subparagraph{Entropy}
As for \cite{NRAM:2016}, we also noticed that the network can fix the pointer in some value. Although it could be an advantage in some cases, however, if this happens too early in the training, it could forces the network to stay fixed on some pointer with a very small chances of change. Hence, to alleviate this condition, we give to the network a sort of a ``entropy bonus'' which decreases over time. This entropy is computed for each generated distribution\footnote{We have identified these distributions be those in the memory.} by the neural network with the Shannon entropy and is multiplied for the following coefficient that decreases over time
\begin{equation}
	E = e * d^{t}
\end{equation}
where $e$ is the entropy coefficient, $d$ is the entropy coefficient's decay and $t$ is the timestep. After that, the computed value is subtracted from timestep cost.

\subparagraph{Curriculum learning}\label{subpar:curriculum-learning}
As noticed by \cite{Bengio2009CurriculumL} and \cite{Zaremba2014LearningTE}, curriculum learning is crucial for train very deep neural networks. Hence, as in the original paper we used the curriculum learning from \cite{Zaremba2014LearningTE}.

For each task we have defined manually a set of increasing difficulties, where a difficulty $d$ is defined as a tuple containing the length of working sequence and the number of timesteps of running. During the training, a difficulty is selected according to the current difficulty $D$ as follows:
\begin{itemize}
	\item{with probability 10\%: pick $d$ randomly from the set of all difficulties according to a uniform distribution.}
	\item{with probability 25\%: pick $d$ randomly from the set $[1, D + e]$ according to a uniform distribution, with $e$ generated from a geometric distribution with a success probability of 0.5.}
	\item{with probability 65\%: set $d = D + e$ where $e$ is sampled as above.}
\end{itemize}
The increasing of difficulty happens when the error rate $1 - \frac{c}{m}$, where $c$ represents the correct modified cells and $m$ the cells that should be modified, goes below an hyperparameter $\lambda$, decided by the user. Furthermore, we ensure that the successive increasing of the difficulty is separated by some number of generations.


\section{How we did it}
The implementation of NRAM where it is trained is similar between the two solutions, differentiating only for the training algorithm and for some specific technique (for example, Gradient Clipping that in the implementation with DENN is useless). Hence, leaving out these specifics of the framework we speak only about the NRAM implementation which is presented for simplicity as pseudocode\footnote{The entire code is available at \href{https://github.com/Gabriele91/DENN-LITE/tree/nram}{https://github.com/Gabriele91/DENN-LITE/tree/nram}}.\newline
The principal block of NRAM can be seen in Algorithm \ref{alg:nram-main-block}. The execution starts with the setting of some variables, such as cost variable and the constants used for the computing of the entropy. The main cycle is at Line \ref{lst:nram-timesteps} - in every timestep the network releases all the coefficients, i.e. the circuit connections, at Line \ref{lst:nram-nn-predict}. At Line \ref{lst:nram-run-circuit} is executed the circuit, with the Algorithm \ref{alg:nram-run-circuit} - it returns the willingness of terminate the execution with which is computed from Line \ref{lst:nram-cost-1} to \ref{lst:nram-cost-2} the timestep cost, added later to the total cost. The ``magic'' of NRAM happens in Algorithm \ref{alg:nram-run-circuit}, where the circuit is executed. From Line \ref{lst:nram-exec-circuit-1} to \ref{lst:nram-exec-circuit-2}, each gate in the list \textbf{Gates} is executed, getting an input\footnote{Depending by the type - Costant gate produce a constant output without getting an input, unary and binary gate gate get, respectively, one and two input.} and producing an output, which might be accessed later by another gate or register. From Line \ref{lst:nram-up-reg-1} to \ref{lst:nram-up-reg-2}, the registers are updated. As it can be seen in the Lines \ref{lst:nram-avg-1}, \ref{lst:nram-avg-2}, \ref{lst:nram-avg-3} and \ref{lst:nram-avg-4}, the selection of the input of a gate and the new content of the register is made through the function in the Algorithm \ref{alg:nram-average} which do a weighted average. An example of gate is visible in Algorithm \ref{alg:nram-gate-add} - the \textbf{add} gate compute the summation of two number represented as two probability distributions. Finally, the functions \textsc{GetGateCoefficient} and \textsc{GetRegisterCoefficient} are purely conceptual functions which indicates a method with which the coefficients are acquired.\newline\newline
The pseudocode of the implementation of the NRAM used to test the generalization capacity of the discovered ANNs is showed in Algorithm \ref{alg:nram-main-block-test}. For each test, defined by the size of the memory and the timesteps, is executed the NRAM for each sample - with the results at Line \ref{lst:nram-test-print-circuit} is written to file the circuits generated during the execution. At Line \ref{lst:nram-test-print-memories} and at Line \ref{lst:nram-test-print-error-rate}, when all the samples of a difficulty are managed, are printed to an image the memories and to a csv the error rate reached by NRAM for the current difficulty. 


\subsubsection{Datasets}\label{subsubsec:nram-dataset}
The datasets used during the training are generated programmaticaly at runtime for each tasks. With a procedure are generated a batch of samples composed by the initial memories, the expected memories, the cost mask and the error rate masks. The initial memories are those manipulated by NRAM during the execution, along the registers, and which are compared to the expected memories for cost calculation. Remembering how a classification problem is defined, from an high level the $x$ is formed by the Registers and ``Initial memory'' and the $y$ is the ``Desired Memory''. The datasets are generated only at the start of the training of the NRAM, except when the Curriculum Learning is active in which are re-generated to every change of difficulty.

\begin{algorithm}
	\begin{algorithmic}[1]
		\Function{NRAM}{NeuralNetwork, BatchSize, Regs, IMem, OMem, CostMask, MaxInt, T, Gates}
			\State{Cost = 0.0}			
            	\State{ProbIncomplete = 1.0}
            	\State{CumProbComplete = 0.0};
            	\State{$p_T = 1.0$}
            	\State{EntropyCoeff = 0.1}
            	\State{EntropyDecay = 0.999}
			\For{$t\in[1, \dots, T]$}\label{lst:nram-timesteps}
				\State{Conf = \Call{NeuralNetwork.predict(Regs.ProbZero())}{}}\label{lst:nram-nn-predict}
				\State{$\{f_i, \textrm{Regs}, \textrm{IMem}\}$ = \Call{RunCircuit}{Gates, Conf, Regs, IMem, MaxInt}}\label{lst:nram-run-circuit}
				\State{}
				\If{t == T}\label{lst:nram-cost-1}
					\State{$p_T$ = 1 - CumProbComplete}
				\Else
					\State{$p_T$ = $f_i$ * ProbIncomplete}
				\EndIf
				\State{}
               	\State{CumProbComplete = CumProbComplete + $p_t$}
                	\State{ProbIncomplete = ProbIncomplete * ($1 - f_i$)}
				\State{}
                	\State{EntropyWeight = $\textrm{EntropyCoeff}*\textrm{EntropyDecay}^t$}
                	\State{EntropyCost = EntropyCost+\Call{Entropy}{IMem} * EntropyWeight}
				\State{}
                	\State{Cost += \Call{CalculateCost}{InMem, OutMem, CostMask} - EntropyCost}\label{lst:nram-cost-2}
			\EndFor{}
			\State{}
			\State{\Return Cost}
		\EndFunction
	\end{algorithmic}
	\caption{Pseudocode of the main block of NRAM.}\label{alg:nram-main-block}
\end{algorithm}

\begin{algorithm}
	\begin{algorithmic}[1]
		\Function{RunCircuit}{Gates, Conf, Regs, IMem, MaxInt}
			\State{RegsAndOutput = \Call{Copy}{Regs}}
			\State{}
			\ForEach{$g\in \textrm{Gates}$}\Comment{Run of the circuit}\label{lst:nram-exec-circuit-1}
				\If{\Call{g.isConstant}}
					\State{GateResult = g.\Call{Value}{}}
				\ElsIf{\Call{g.isUnary}}
					\State{Coeff = \Call{GetGateCoefficient}{Conf, g}}
					\State{InputValue = \Call{Avg}{RegsAndOutput, Coeff}}\label{lst:nram-avg-1}
					\State{GateResult = g.\Call{execute}{InputValue, IMem, MaxInt}}	
				\Else
					\State{(CoeffA, CoeffB) = \Call{GetGateCoefficient}{Conf, g}}
					\State{InputValueA = \Call{Avg}{RegsAndOutput, CoeffA}}\label{lst:nram-avg-2}
					\State{InputValueB = \Call{Avg}{RegsAndOutput, CoeffB}}\label{lst:nram-avg-3}
					\State{GateResult = g.\Call{execute}{GateValueA, GateValueB, IMem, MaxInt}}	
				\EndIf
				\State{RegsAndOutput = \Call{Concatenate}{Regs, GateValue}}
			\EndFor{}\label{lst:nram-exec-circuit-2}
			\newline
			\For{$i\in[1, \dots, |\textrm{Regs}|]$}\Comment{Update of the registers}\label{lst:nram-up-reg-1}
				\State{Coeff = \Call{GetGateCoefficient}{Conf, i}}
				\State{NewValue = \Call{Avg}{RegsAndOutput, Coeff}}\label{lst:nram-avg-4}
				\State{Regs[i] = NewValue}
			\EndFor{}\label{lst:nram-up-reg-2}
			\State{}
			\State{\Return{(Conf[|Conf| - 1], Distributions)}}
		\EndFunction
	\end{algorithmic}
	\caption{Pseudocode of the run circuit function.}\label{alg:nram-run-circuit}
\end{algorithm}


\begin{algorithm}
	\begin{algorithmic}[1]
		\Function{Avg}{RegsOut, Coefficient} 
			\State{\Return{RegsOut * Coefficient}}
		\EndFunction
	\end{algorithmic}
	\caption{Pseudocode of the avg function}\label{alg:nram-average}
\end{algorithm}

\begin{algorithm}
	\begin{algorithmic}[1]
		\Function{Add}{A, B, IMem, MaxInt}
			\State{C = \Call{Zero(MaxInt)}{}}\Comment{Matrix of zeros where is stored the Add's output} 
			\For{$i \in [0, \textrm{MaxInt} - 1]$}
				\For{$j \in [0, \textrm{MaxInt} - 1]$}
					\State{C[j] += A[i] * B[(j - i) \% MaxInt]}
				\EndFor{}
			\EndFor{}
			\State{\Return{C}}
		\EndFunction
	\end{algorithmic}
	\caption{Example of NRAM gate.}
	\label{alg:nram-gate-add}
\end{algorithm}

\begin{algorithm}
	\begin{algorithmic}[1]
		\Function{NramTest}{NeuralNetwork, BatchSize, Tests, Gates}
			\For{$\{$MaxInt, Timestep$\} \in $Tests}
				\State{$\{$Regs, IMem, OMem, CostMask$\}$ = \Call{GenerateBatch}{BatchSize, MaxInt, Timestep}}
				\For{$s \in [1, \dots, $BatchSize$]$}
					\State{SampleIMem = IMem[s]}
					\State{SampleOMem = OMem[s]}
					\State{SampleRegs = Regs[s]}
					\For{$t \in [1, \dots, $Timestep$]$}
						\State{Conf = \Call{NeuralNetwork.predict}{Regs.ProbZero()}}
						\State{$\{$SampleRegs, SampleIMem$\}$ = \Call{RunCircuit}{Gates, Conf, SampleRegs, SampleIMem, MaxInt}}
					 	\State{\Call{PrintCircuitToFile}{Conf}}\label{lst:nram-test-print-circuit}
					\EndFor{}
				\EndFor{}
				\State{\Call{PrintMemoriesToFile}{SampleIMem}}\label{lst:nram-test-print-memories}
				\State{\Call{PrintErrorRateToFile}{SampleIMem, SampleOMem, CostMask}}\label{lst:nram-test-print-error-rate}
			\EndFor{}
		\EndFunction
	\end{algorithmic}
	\caption{Pseudocode of the NRAM tester.}\label{alg:nram-main-block-test}
\end{algorithm}