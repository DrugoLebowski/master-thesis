
% Parla del fatto che non tutti i vari algoritmi sono stati appresi dal controllore ipotizzando una motivazione e del fatto che i test con un sottoinsieme delle porte ha evidenziato una migliore capacit√† da parte del differential evolution di convergere in una soluzione.

% In ``Future works`` il fatto che la NRAM potrebbe essere migliorata inserendo un sistema, ricollegandosi al discorso precedente, per selezionare un sottoinsieme di porte migliore, dove con migliore si intende che direzioni il Differential Evolution a convergere ad un minimo globale in meno generazioni.
In this work a version of NRAM with Differential Evolution as optimization engine has been presented. In particular the DENN framework has been used instead of ADAM optimization algorithm. 

%In order to make the fairest comparison possible 
We implemented two versions of the system: the first is a re-factoring and an extension of the project of \hyperref[https://github.com/gibiansky/experiments/tree/master/nram]{Andrew Gibiansky}, an implementation of NRAM written in Python with Theano and using ADAM algorithm; the second one is the new version proposed in this thesis, it is written in C++ and uses DENN as optimization framework. 

As seen in the experiments, we concluded that Differential Evolution and DENN behave well with this type of machine and with large neural networks. As seen previously, in the Access, Increment and Reverse tasks, solutions that generalize perfectly are reached, also without the use of Curriculum Learning. This show that the research made by Differential Evolution is more effective respect to the one made by Gradient Descent and ADAM for this kind of problems. 

%Perhaps it is difficult to provide a final interpretation of these results because more tests and run have to be done. 
Considering the data obtained so far we can conclude that, apart the case of JADE/DEGL/1/bin + CL, the best performing variants are the ones with the Current-to-pbest mutation method. Despite this, it is clear that the contribution of the Curriculum Learning is crucial in some cases like in the Increment task.

We observed also that the solutions found with DENN return more intuitive circuits compared to those in \cite{NRAM:2016}, showing that the exploitation and exploration done by DENN is more effective respect to those executed by Gradient Descent.

\section{Future works}
For the future we are going to complete all the experiments also with new tasks. Moreover, since the current structure of the gates is too stringent making the execution excessively static with the forced use of all them, one of our objective is to upgrade the NRAM with a more advanced system for a dynamic selection of the gates. Finally, we are planning to rewrite a enhanced NRAM model which does not require the differentiability condition; in this case we will loose the possibility to use the gradient descent method to train the controller, but the model could result simpler and its controller can be anyway trained with an evolutionary optimization algorithm.